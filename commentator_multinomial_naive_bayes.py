# -*- coding: utf-8 -*-
"""Commentator - Multinomial Naive Bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gTkXnx-wgGIfE2Erel1MO0oO3YsMLGuj

### **Importing Pandas Library and Training Dataset**
"""

import pandas as pd
df=pd.read_csv("/content/mod_train.txt", header=None, sep=" ", names=["Comment","Emotion"], encoding="utf-8",nrows=10000)
tdf=pd.read_csv("/content/test.txt", header=None, sep=";", names=["Comment","Emotion"], encoding="utf-8",nrows=1000)
X_train=df['Comment'].tolist()
print(len(X_train))
X_accu=tdf['Comment'].tolist()
y_train=df['Emotion'].tolist()
y_result=tdf['Emotion'].tolist()

X_train

y_train

"""### **Importing NLTK Tokenizer, Stemmer and Stopwords Remover**"""

from nltk.tokenize import RegexpTokenizer
from nltk.stem.porter import PorterStemmer
from nltk.corpus import stopwords 
import nltk
nltk.download('stopwords')

"""### **Creating Objects for NLTK Tokenizer, PorterStemmer and Stopwords Remover**"""

tokenizer=RegexpTokenizer(r'\w+')
en_stopwords=set(stopwords.words('english'))
ps=PorterStemmer()
print(en_stopwords)

"""### **Function for Data Cleaning (Tokenizing, Stopwords Removal, Stemming)**"""

def clean_text(text):
  text=text.lower()
  tokens=tokenizer.tokenize(text) #Tokenizing
  new_token=[token for token in tokens if token not in en_stopwords] #Stopwords Removal
  stemmed_tokens=[ps.stem(tokens) for tokens in new_token] #Stemming
  clean_texts=" ".join(stemmed_tokens)
  return clean_texts

"""### **Data Cleaning (Tokenizing, Stopwords Removal, Stemming) for Training Dataset**"""

X_clean=[clean_text(ct) for ct in X_train]

X_clean

"""### **Importing CountVectorizer and Creating Object**"""

from sklearn.feature_extraction.text import CountVectorizer
cv=CountVectorizer(ngram_range=(1,2))

"""



### **Using CountVectorizer to transform Training Data to Vectors**"""

X_vec = cv.fit_transform(X_clean).toarray()

X_vec

"""### **Using TfidfTransformer to transform Training Data based on Term Frequency and Inverse Document Frequency**"""

from sklearn.feature_extraction.text import TfidfTransformer
tfidf=TfidfTransformer()

X_vec_tfidf=tfidf.fit_transform(X_vec).toarray()

X_vec_tfidf

"""### **Building Multinomial Naive Bayes Model**"""

from sklearn.naive_bayes import MultinomialNB
mn=MultinomialNB()

"""### **Training Multinomial Naive Bayes Model using Training Dataset**"""

mn.fit(X_vec_tfidf,y_train)

"""### **YouTube Comment Extraction for Testing Multinomial Naive Bayes Model**"""

def youtube_request(video_id,pgtoken):
  #credentials
  api_key ="AIzaSyByzbpRtN9DSkL8RvcJBL1KtOwCdlzItMk"

#build a resource for youtube
  resource = build('youtube', 'v3', developerKey=api_key)
  
  if pgtoken == 0:
#create a request to get 100 comments on the video
   request = resource.commentThreads().list(
                            part="snippet",
                            videoId=video_id,
                            maxResults= 100,   #get 100 comments
                            order="relevance")  
  else:
   request = resource.commentThreads().list(
                            part="snippet",
                            videoId=video_id,
                            maxResults= 100,#get 100 comments
                            pageToken=pgtoken,
                            order="relevance",)  
#execute the request
  response =request.execute()
  return response

from googleapiclient.discovery import build

video_id= str(input("Enter Video ID:"))

X_test=[]

for counter in range(5):
  if counter == 0:
    res=youtube_request(video_id,0)
    nxtpgtoken=res["nextPageToken"]
  else:
    res=youtube_request(video_id,nxtpgtoken)  
    nxtpgtoken=res["nextPageToken"]
    #get first 100 items to extract 100 comments
  items = res["items"][:100]
  for item in items:
    item_info = item["snippet"]
    #the top level comment can have sub reply comments
    topLevelComment = item_info["topLevelComment"]
    comment_info = topLevelComment["snippet"]
    X_test.append(comment_info["textDisplay"])

X_test

"""### **Data Cleaning (Tokenizing, Stopwords Removal, Stemming) and Vector Transformation of YouTube Comments**"""

Xt_clean=[clean_text(ct) for ct in X_test]
Xa_clean=[clean_text(ct) for ct in X_accu]

Xt_clean

Xt_vec = cv.transform(Xt_clean).toarray()
Xa_vec = cv.transform(Xa_clean).toarray()
Xt_vec_tfidf=tfidf.transform(Xt_vec).toarray()
Xa_vec_tfidf=tfidf.transform(Xa_vec).toarray()

Xt_vec_tfidf

"""### **Prediction of Sentiment from YouTube Comments by Multinomial Naive Bayes Model**"""

ct_prediction=mn.predict(Xt_vec_tfidf)

ct_prediction

test_prediction=mn.predict(Xa_vec_tfidf)

test_prediction

"""### **Extraction of Results Predicted by Multinomial Naive Bayes Model**"""

sad,angr,lv,sur,fr,joy=0,0,0,0,0,0
for result in ct_prediction:
  if result == "sadness":
    sad+=1
  elif result == "joy":
    joy+=1
  elif result == "anger":
    angr+=1
  elif result == "fear":
    fr+=1
  elif result == "surprise":
    sur+=1
  else:
    lv+=1    
print("Sad Comments:",sad)
print("Happy Comments:",joy)
print("Angry Comments:",angr)
print("Fearful Comments:",fr)
print("Love Comments:",lv)
print("Surprise Comments:",sur)

"""### **Accuracy of Results Predicted by Multinomial Naive Bayes Model**"""

from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
print("Accuracy:",accuracy_score(test_prediction,y_result),end="\n\n\n")
possible_values=["sadness","joy","anger","fear","surprise","love"]
print("Classification Report:",end="\n\n")
print(classification_report(test_prediction,y_result,target_names=possible_values),end="\n\n\n")
print("Confusion Matrix:",end="\n\n")
print(confusion_matrix(test_prediction,y_result))